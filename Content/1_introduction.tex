In 2012 one of Google's cofounders, Sergey Brin, in an interview says: ``You'll
ride in robot cars within 5 years''. That is in just one year from now and while
it might seem as if that is a bit too optimistic, robot cars, or self-driving
cars as they are more often called, are not far from realistic. Google's
self-driving cars have together driven more than 1.5 million miles.

The quick development of self-driving cars is driven by the many advantages that
might be gained from getting humans off the road. In their annual report on
traffic safety WHO points out that 1.2 million people die each year in traffic
accidents~\cite{worldhealthorganization_2015_global_gsrors2sadoa}. And this is
just looking at the traffic accidents that are fatal, low- and middle-income
countries lose approximately 3\% of their GDP as a result of traffic accidents.
Studies also show that a majority of these accidents seem to be the cause
``human error''~\cite{relative_trfoudas}, something an self-driving car would
not be prone to do.

Further motivation to the gains from self-driving cars are described by Howard
in~\cite{howard_science_smbarotrtmiotdc}, where he mentions the ability for
disabled to use cars and the ability to revitalize failing cities.

But self-driving cars suffers from the problem of responsibility: who is to be
held responsible when a self-driving car crashes? Or makes a seemingly odd
decision? Some argue that the temporary solution is to allow the driver to
intervene, temporarily taking control of the car to handle the situation.
However, as Goodall points out this has several problems resulting in that the
probability is high the driver will unable or unwilling to take control ---
making the car forced to take action anyway~\cite{goodall_2014_machine_meaav}.

It therefore seems that the cars must be capable of taking all decisions
themselves and that the problem of responsibility remain. This is a problem that
needs solving, because as pointed out by Merchant et.\ al.:
``Cars crash. So too will autonomous vehicles [$\ldots$]''.

It is not hard to imagine a scenario where a car will crash and in deciding what
to do must take an ethical decision. As an example let's use a variation of the
commonly used Trolley problem~\cite{2016_trolley_tp} outlined by Goodall:
The car is driving on a small bridge and in the opposite lane is a bus, which
suddenly turns towards the car's lane. The car then has three possible actions:
\begin{enumerate}
\item Drive off the bridge, guaranteeing a serious accident for the car;
\item Drive straight on towards a head-on collision with the bus, causing a less
  serious accident for both the car and bus;
\item Or attempt to drive past the bus with the possibility of a avoiding a
  crash, but a probability of much more serious injuries for the passengers of
  both the bus and the car.
\end{enumerate}
It is not hard to imagine that a similar scenario will occur reality, forcing
the car to make a decision with moral implications --- does it sacrifice the
driver in favor of the probably more passengers of the bus?