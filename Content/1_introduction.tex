En sak som diskuteras och arbetas mycket med just nu är så självkörande bilar, alltså bilar som styrs av ett autonomt system istället för en människa. Ett autonomt system är ett system som kan utföra en uppgift utan mänsklig hjälp. Flertalet företag håller just nu på att utveckla bilar som är delvis eller fullt autonoma. Mest intressant ur ett filosofiskt perspektiv är de bilar som NHTSA definierar som nivå 4: fullständigt autonoma bilar som inte kräver någon mänsklig hjälp \cite{nhtsa}. Flera företag arbetar med autonoma bilar på nivå 4, och exempelvis Googles autonoma bilar har kört över 1 miljon kilometer de senaste åren \cite{googlecars}. Sergey Brin, en av Googles grundare, har i en intervju år 2012 sagt "You'll ride in robot cars within 5 years" \cite{sergeybrin}.

Autonoma system kan minimera mänskliga fel och därmed minska riskerna för människor men introducerar andra, främst moraliska, problem. Systemen tillåter oss att bestämma vad ett korrekt agerande givet en viss situation är, på gott och ont. I fallet med autonoma bilar är det ofta positivt: exempelvis kan vi bestämma att bilen ska bromsa vid gult ljus, inte accelerera för att hinna innan det blir rött. Det blir dock svårare när det krävs att systemen tar beslut i moraliskt komplexa situationer.

Ett exempel på ett problem en autonom bil kan ställas inför är en variant av det kända "trolley problem" \cite{trolleyproblem}; Noah J. Goodall presenterar en situation där bilen kör på en trång bro och i motsatt riktning kommer en buss, som plötsligt svänger över mot bilens sida av vägen \cite{goodall2014ethical}. Bilen ställs då inför tre möjliga alternativ:
\begin{enumerate}
    \item Svänga av bron och garantera en allvarlig olycka med enbart bilen;
    \item Frontalkrocka med bussen och orsaka en mindre allvarlig olycka involverande både bil och buss;
    \item Eller försöka köra förbi bussen till höger om den med en möjlighet att undvika en krock, men med en risk för en värre olycka med större skaderisk för passagerarna i både bilen och bussen.
\end{enumerate}

Situationen är en form av Kobayashi Maru test \cite{kobayashimaru}, ett scenario där alla möjliga alternativ är dåliga på olika sätt. En människa som ställs inför problemet följer troligen sin egna och individuella moral för att välja hur den agerar. För ett autonomt system krävs dock att vi på förhand programmerar systemet och bestämmer hur det ska agera, vilket kräver att vi ger systemet någon form av moral.

Autonoma bilar kommer ställas inför verkliga varianter av Kobayashi Maru testet. Noah J. Goodall visar i en artikel att bilarna kommer, på grund av egna fel eller yttre omständigheter som inte går att kontrollera, hamna i olyckor \cite{machineethics}. Med tanke på  det stora antalet bilar – i exempelvis USA hade 190 miljoner personer körkort år 2000 \cite{licenseddrivers} och körde i snitt 21,7 tusen kilometer per år \cite{avgmiles} – kommer dessa scenarion troligen inte vara ovanliga.