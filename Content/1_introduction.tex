In 2012 one of Google's cofounders, Sergey Brin, states in an interview: ``You'll
ride in robot cars within 5 years''. That is just one year from now, and while
it might seem as if that is a bit too optimistic, robot cars, or self-driving
cars as they are more often called, are not very far away. Just Google's own
self-driving cars have together driven more than 1.5 million
miles~\cite{google_gscp}.

Self-driving cars have seen a quick development in recent years, a development
driven by the many advantages the cars offer, the primary of which might be
reducing traffic accidents. Several studies show that a majority of traffic
accidents seem to be the cause of ``human error''~\cite{relative_trfoudas},
accidents self-driving cars could avoid. Add to that, that 1.2 million people
die each year in traffic
accidents~\cite{worldhealthorganization_2015_global_gsrors2sadoa} and that they
cost us considerably --- low- and middle-income countries lose approximately 3\%
of their GDP as a result of traffic accidents --- and you should have sufficient
motivation to why self-driving cars would be beneficial.

There are also many more advantages to be gained from self-driving cars and
Howard describe two important ones in~\cite{howard_science_smbarotrtmiotdc},
where he mentions the ability for disabled to use cars and the ability to
revitalize failing cities.

However, self-driving cars suffer from the problem of responsibility: who is to
be held responsible when a self-driving car crashes? Or when it makes a
seemingly odd decision? One argument is that a temporary solution is to allow the
driver to intervene, temporarily taking control of the car to handle the
situation. However, as pointed out in~\cite{goodall_2014_machine_meaav}, this is
problematic and it is likely not realistic.

It therefore seems as if the cars must be capable of taking all decisions
themselves and thus that the problem of responsibility remains. This is a
problem that needs to be solved, because as pointed out by Merchant et.\ al.:
``Cars crash. So too will autonomous vehicles
$\ldots$''~\cite{marchant_2012_coming_ccbavatlst}.

It is not hard to imagine a scenario where a car will be forced to make an
ethical decision. As an example let's use an example scenario outlined by
Goodall in~\cite{goodall_2014_ethical_edmdavc}:
The car is driving on a small bridge and in the opposite lane is a bus, which
suddenly turns towards the car's lane. The car is presented with three possible
actions:
\begin{enumerate}
\item Drive off the bridge, guaranteeing a serious accident for the car;
\item Drive straight towards a head-on collision with the bus which would cause
  a less serious accident for both the car and bus;
\item Or attempt to drive past the bus with the possibility of avoiding a crash,
  but a probability of much more serious injuries for the passengers of both the
  bus and the car.
\end{enumerate}

It is not hard to imagine that similar scenarios will occur in reality, forcing
the car to make decisions with moral implications --- does it sacrifice the
driver of the passengers of the bus? Does it try to save the driver? Or does it
try to avoid a collision, but at a higher risk?